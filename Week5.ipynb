{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c3f1e7",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc25aed",
   "metadata": {},
   "source": [
    "1. Importance of Handling Missing Data\n",
    "Handling missing data is crucial because:\n",
    "\n",
    "Maintains Data Integrity: Missing values can lead to biased or incorrect conclusions.\n",
    "Ensures Model Performance: Many machine learning models cannot handle missing values, leading to errors or poor predictions.\n",
    "Prevents Data Loss: Simply dropping missing values can lead to losing valuable information if done excessively.\n",
    "Improves Accuracy: Properly handling missing data reduces noise and improves the reliability of insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5174a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count:\n",
      " pclass          0\n",
      "survived        0\n",
      "name            0\n",
      "sex             0\n",
      "age           263\n",
      "sibsp           0\n",
      "parch           0\n",
      "ticket          0\n",
      "fare            1\n",
      "cabin        1014\n",
      "embarked        2\n",
      "boat          823\n",
      "body         1188\n",
      "home.dest     564\n",
      "dtype: int64\n",
      "\n",
      "Are there any missing values in the dataset? True\n",
      "\n",
      "Percentage of Missing Values:\n",
      " pclass        0.000000\n",
      "survived      0.000000\n",
      "name          0.000000\n",
      "sex           0.000000\n",
      "age          20.091673\n",
      "sibsp         0.000000\n",
      "parch         0.000000\n",
      "ticket        0.000000\n",
      "fare          0.076394\n",
      "cabin        77.463713\n",
      "embarked      0.152788\n",
      "boat         62.872422\n",
      "body         90.756303\n",
      "home.dest    43.086325\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"titanic_data_2.csv\")\n",
    "\n",
    "# Check for missing values count\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Check if there are any missing values in the dataset\n",
    "any_missing = df.isnull().values.any()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Visualize missing values using a heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Missing Values Count:\\n\", missing_values)\n",
    "print(\"\\nAre there any missing values in the dataset?\", any_missing)\n",
    "print(\"\\nPercentage of Missing Values:\\n\", missing_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed213a7",
   "metadata": {},
   "source": [
    "2. Identifying Missing Values in the Titanic Dataset\n",
    "\n",
    "The dataset contains missing values in several columns:\n",
    "\n",
    "Age: 263 missing values\n",
    "Fare: 1 missing value\n",
    "Cabin: 1014 missing values\n",
    "Embarked: 2 missing values\n",
    "Boat: 823 missing values\n",
    "Body: 1188 missing values\n",
    "Home/Destination: 564 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e20bb9",
   "metadata": {},
   "source": [
    "3. Methods for Handling Missing Data in Pandas\n",
    "\n",
    "- dropna() – Removes rows or columns with missing values.\n",
    "  Use when missing data is minimal and dropping it won't significantly reduce dataset size.\n",
    "- fillna(value) – Replaces missing values with a specified value (e.g., mean, median).\n",
    "  Use when data loss is undesirable, and replacing values makes sense.\n",
    "- interpolate() – Fills missing values using interpolation (linear, polynomial, etc.).\n",
    "  Useful for time-series or continuous numerical data.\n",
    "- ffill() / bfill() – Forward or backward fills missing values with adjacent data.\n",
    "  Suitable for ordered datasets where previous or next values are meaningful replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e6de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Age' after filling: 0\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate the median age\n",
    "median_age = df['age'].median()\n",
    "\n",
    "# Fill missing values in 'Age' column with the median age\n",
    "df['age'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Verify that missing values in 'Age' are handled\n",
    "print(\"Missing values in 'Age' after filling:\", df['age'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b28683",
   "metadata": {},
   "source": [
    "Clean the Titanic dataset by filling missing values in the 'Age' column\n",
    "with the median age of the passengers, and explain your choice.\n",
    "\n",
    "To clean the dataset, we will fill missing values in the 'Age' column with the median age of passengers. \n",
    "The median is a better choice than the mean because it is less affected by outliers. This ensures that the filled values \n",
    "represent the central tendency of the data without being skewed by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b047802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "#5. Explain how duplicate records can impact analysis.\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_count)\n",
    "\n",
    "# Remove duplicate records\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486ccc2",
   "metadata": {},
   "source": [
    "Duplicate records can negatively affect data analysis in several ways:\n",
    "\n",
    "Skewed Statistics: Duplicates can inflate counts and averages, leading to misleading conclusions.\n",
    "Incorrect Model Training: In machine learning, duplicates can cause bias and overfitting, reducing model accuracy.\n",
    "Inaccurate Reporting: Business decisions based on incorrect data may lead to poor strategic choices.\n",
    "Increased Storage & Processing Costs: More duplicates mean unnecessary memory usage and slower computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61932112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Number of duplicate rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "#6. Demonstrate how to detect and remove duplicate rows using Pandas’ “drop_duplicates()” method.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"titanic_data.csv\")\n",
    "\n",
    "# Detect duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_count)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Verify duplicate removal\n",
    "duplicate_count_after = df_cleaned.duplicated().sum()\n",
    "print(\"Number of duplicate rows after removal:\", duplicate_count_after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e437d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows before removal: 0\n",
      "No duplicates found in the dataset.\n",
      "Cleaned dataset saved as 'titanic_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "#7. Check the Titanic dataset for duplicates and remove any found, if applicable.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"titanic_data.csv\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows before removal:\", duplicate_count)\n",
    "\n",
    "# Remove duplicates if any exist\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Duplicates were found and removed.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the dataset.\")\n",
    "\n",
    "# Save the cleaned dataset (optional)\n",
    "df.to_csv(\"titanic_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset saved as 'titanic_cleaned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fad2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Perform every other data cleaning step to prepare the data for analysis\n",
    "# (e.g., Convert the passenger class column from numeric values to categorical labels)\n",
    "\n",
    "def clean_titanic_data(df):\n",
    "    # Convert 'pclass' from numeric to categorical labels\n",
    "    df['pclass'] = df['pclass'].map({1: '1st class', 2: '2nd class', 3: '3rd class'})\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['age'].fillna(df['age'].median(), inplace=True)\n",
    "    df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
    "    df['fare'].fillna(df['fare'].median(), inplace=True)\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    df.drop(columns=['cabin', 'boat', 'body', 'home.dest'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7978c",
   "metadata": {},
   "source": [
    "9. Define what outliers are and discuss how they can impact data analysis\n",
    "\n",
    "Outliers are extreme values that deviate significantly from the rest of the dataset.\n",
    "They can skew statistical summaries, affect machine learning model performance, and mislead interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a7bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Demonstrate how to identify outliers in a numerical column like 'Fare' in the Titanic dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c6ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Choose a strategy (e.g., capping, removing) for handling outliers in the 'Fare' column, and explain why you \n",
    "#made this choice Strategy: Capping outliers to the upper bound to prevent extreme values from skewing analysis\n",
    "\n",
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf3b97",
   "metadata": {},
   "source": [
    "## 2. Data Transformation and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee107b",
   "metadata": {},
   "source": [
    "1. Importance of Ensuring the Correct Data Types in a DataFrame\n",
    "Ensuring correct data types in a DataFrame is crucial because:\n",
    "\n",
    "Memory Efficiency: Using appropriate data types reduces memory usage, making computations faster.\n",
    "Accurate Calculations: Numeric operations require correct data types to avoid errors (e.g., string-based numbers may not allow mathematical operations).\n",
    "Correct Data Handling: Some functions behave differently based on data types (e.g., categorical vs. numerical).\n",
    "Ensuring Data Integrity: Misclassified data types (e.g., dates stored as strings) can lead to incorrect interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e712969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#2. Examine the data types of each column in the Titanic dataset using the“dtypes” attribute and convert any columns that \n",
    "#are incorrectly typed (e.g., convert 'PassengerId' to a string type).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"titanic_data_2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display column names\n",
    "print(df.columns)\n",
    "\n",
    "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "df.columns = df.columns.str.lower()  # Convert to lowercase for consistency\n",
    "\n",
    "\n",
    "\n",
    "#PassengerId should be a string because it is an identifier, not a numeric value.\n",
    "#Pclass (Passenger class) is categorical, so converting it to a string makes sense for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1c0a0",
   "metadata": {},
   "source": [
    "3. How Transforming Data Leads to New Insights\n",
    "Data transformation helps extract meaningful insights by:\n",
    "\n",
    "Creating Derived Features: New features like \"FamilySize\" can reveal survival patterns.\n",
    "Standardizing Data: Normalization helps in machine learning applications.\n",
    "Simplifying Analysis: Converting complex data (e.g., timestamps) into readable formats aids interpretation.\n",
    "Enhancing Comparisons: Features like \"FarePerPerson\" make economic class comparisons easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a02733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sibsp  parch  FamilySize\n",
      "0      0      0           1\n",
      "1      1      2           4\n",
      "2      1      2           4\n",
      "3      1      2           4\n",
      "4      1      2           4\n"
     ]
    }
   ],
   "source": [
    "#4. Create a new feature called “FamilySize” by summing the 'SibSp' and 'Parch' columns, adding 1 \n",
    "#(to include the passenger themselves).\n",
    "\n",
    "df['FamilySize'] = df['sibsp'] + df['parch'] + 1\n",
    "print(df[['sibsp', 'parch', 'FamilySize']].head())\n",
    "\n",
    "#The new FamilySize feature represents the total number of people in a passenger's family, including themselves:\n",
    "#This feature helps analyze survival likelihood—larger families may have struggled more to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e848ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fare  FamilySize  FarePerPerson\n",
      "0  211.3375           1       211.3375\n",
      "1  151.5500           4        37.8875\n",
      "2  151.5500           4        37.8875\n",
      "3  151.5500           4        37.8875\n",
      "4  151.5500           4        37.8875\n"
     ]
    }
   ],
   "source": [
    "#5. Create another feature called “FarePerPerson” by dividing the 'Fare' by the new “FamilySize” feature, and explain how \n",
    "#this new feature could provide more insights about passengers' economic backgrounds\n",
    "\n",
    "df['FarePerPerson'] = df['fare'] / df['FamilySize']\n",
    "print(df[['fare', 'FamilySize', 'FarePerPerson']].head())\n",
    "\n",
    "#The FarePerPerson feature helps determine economic standing by dividing total fare by family size:\n",
    "#Insight: A lower FarePerPerson suggests a lower-class passenger, while higher values suggest wealthier passengers.\n",
    "#Why Useful? Wealthier passengers (with high fares) might have had better survival chances due to access to lifeboats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0012ed",
   "metadata": {},
   "source": [
    "## 3. Merging and Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9556d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n",
      "    ID     Name  Score\n",
      "0   3  Charlie     85\n",
      "\n",
      "Concatenated DataFrame:\n",
      "    ID     Name  Score\n",
      "0   1    Alice    NaN\n",
      "1   2      Bob    NaN\n",
      "2   3  Charlie    NaN\n",
      "3   3      NaN   85.0\n",
      "4   4      NaN   90.0\n",
      "5   5      NaN   95.0\n"
     ]
    }
   ],
   "source": [
    "#1. Explain the difference between merging and concatenating DataFrames.\n",
    "\n",
    "# Merging vs. Concatenating DataFrames\n",
    "\n",
    "# Merging combines DataFrames based on a common key (like SQL joins).\n",
    "# Concatenation stacks DataFrames either vertically (adding rows) or horizontally (adding columns).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [3, 4, 5], 'Score': [85, 90, 95]})\n",
    "\n",
    "# Merging (inner join by default)\n",
    "merged_df = pd.merge(df1, df2, on='ID')\n",
    "\n",
    "# Concatenating (stacking DataFrames vertically)\n",
    "concat_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(\"Merged DataFrame:\\n\", merged_df)\n",
    "print(\"\\nConcatenated DataFrame:\\n\", concat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28824056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Shape: (1309, 14)\n",
      "Concatenated DataFrame Shape: (1309, 14)\n"
     ]
    }
   ],
   "source": [
    "#2. Split the Titanic dataset into two subsets: one containing passengerswho survived and one containing passengers who did not \n",
    "#survive.Concatenate these two DataFrames back together using the “concat()” function.\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = pd.read_csv(\"titanic_data_2.csv\")\n",
    "\n",
    "# Splitting dataset into survivors and non-survivors\n",
    "survivors = df[df[\"survived\"] == 1]\n",
    "non_survivors = df[df[\"survived\"] == 0]\n",
    "\n",
    "# Concatenating them back together\n",
    "recombined_df = pd.concat([survivors, non_survivors])\n",
    "\n",
    "# Display the shape to confirm\n",
    "print(\"Original DataFrame Shape:\", df.shape)\n",
    "print(\"Concatenated DataFrame Shape:\", recombined_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "612482d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join:\n",
      "    ID     Name  Score\n",
      "0   3  Charlie     85\n",
      "\n",
      "Left Join:\n",
      "    ID     Name  Score\n",
      "0   1    Alice    NaN\n",
      "1   2      Bob    NaN\n",
      "2   3  Charlie   85.0\n",
      "\n",
      "Right Join:\n",
      "    ID     Name  Score\n",
      "0   3  Charlie     85\n",
      "1   4      NaN     90\n",
      "2   5      NaN     95\n",
      "\n",
      "Outer Join:\n",
      "    ID     Name  Score\n",
      "0   1    Alice    NaN\n",
      "1   2      Bob    NaN\n",
      "2   3  Charlie   85.0\n",
      "3   4      NaN   90.0\n",
      "4   5      NaN   95.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Describe the different types of joins (inner, outer, left, right) used in merging DataFrames.\n",
    "\n",
    "# Different Types of Joins in Merging DataFrames\n",
    "\n",
    "\"\"\"\n",
    "1. INNER JOIN:\n",
    "   - Keeps only the rows where there is a match in both DataFrames.\n",
    "   - Example: If two DataFrames have a common 'ID' column, only IDs that exist in both will be kept.\n",
    "\n",
    "2. LEFT JOIN:\n",
    "   - Keeps all rows from the left DataFrame and only matching rows from the right DataFrame.\n",
    "   - If there is no match, NaN (missing values) will be filled in.\n",
    "\n",
    "3. RIGHT JOIN:\n",
    "   - Keeps all rows from the right DataFrame and only matching rows from the left DataFrame.\n",
    "   - If there is no match, NaN (missing values) will be filled in.\n",
    "\n",
    "4. OUTER JOIN:\n",
    "   - Combines all rows from both DataFrames.\n",
    "   - If a match is not found, missing values (NaN) will be filled in for the non-matching parts.\n",
    "\n",
    "Example:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [3, 4, 5], 'Score': [85, 90, 95]})\n",
    "\n",
    "inner_join = pd.merge(df1, df2, on='ID', how='inner')  # Only matching rows\n",
    "left_join = pd.merge(df1, df2, on='ID', how='left')    # All from df1, matching from df2\n",
    "right_join = pd.merge(df1, df2, on='ID', how='right')  # All from df2, matching from df1\n",
    "outer_join = pd.merge(df1, df2, on='ID', how='outer')  # All rows from both\n",
    "\n",
    "print(\"Inner Join:\\n\", inner_join)\n",
    "print(\"\\nLeft Join:\\n\", left_join)\n",
    "print(\"\\nRight Join:\\n\", right_join)\n",
    "print(\"\\nOuter Join:\\n\", outer_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0758fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pclass  survived                                             name     sex  \\\n",
      "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
      "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
      "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
      "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
      "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
      "\n",
      "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
      "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
      "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
      "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
      "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "\n",
      "                         home.dest  PassengerId Destination  \n",
      "0                     St Louis, MO            0    New York  \n",
      "1  Montreal, PQ / Chesterville, ON            1       Paris  \n",
      "2  Montreal, PQ / Chesterville, ON            2      London  \n",
      "3  Montreal, PQ / Chesterville, ON            3      Berlin  \n",
      "4  Montreal, PQ / Chesterville, ON            4       Tokyo  \n"
     ]
    }
   ],
   "source": [
    "# 4. Suppose you have an additional DataFrame containing information about passengers’ destinations. Demonstrate how to merge \n",
    "#this DataFrame with the Titanic dataset using “merge()” based on a common key like 'PassengerId'.\n",
    "\n",
    "# Creating a mock DataFrame for passenger destinations\n",
    "destinations_df = pd.DataFrame({\n",
    "    'PassengerId': df.index[:10],  # Assuming PassengerId is the index\n",
    "    'Destination': ['New York', 'Paris', 'London', 'Berlin', 'Tokyo',\n",
    "                    'Rome', 'Sydney', 'Madrid', 'Toronto', 'Dublin']\n",
    "})\n",
    "\n",
    "# Merging with Titanic dataset on PassengerId\n",
    "merged_df = pd.merge(df, destinations_df, left_index=True, right_on=\"PassengerId\")\n",
    "\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46c1d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Score\n",
      "0   3  Charlie     85\n"
     ]
    }
   ],
   "source": [
    "# 5. Perform an inner join, and explain how the join type affects the final DataFrame.\n",
    "\n",
    "# Inner Join Effect: Keeps only matching records in both DataFrames\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [3, 4, 5], 'Score': [85, 90, 95]})\n",
    "\n",
    "inner_join = pd.merge(df1, df2, on='ID', how='inner')\n",
    "\n",
    "# Only 'ID' 3 is common, so only this row is kept\n",
    "print(inner_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efd223e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_private_bathroom  has_balcony\n",
      "0.0                   0.0            0.500000\n",
      "                      1.0            1.000000\n",
      "1.0                   0.0            0.666667\n",
      "                      1.0            0.500000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have a DataFrame containing information about passengers' cabin numbers and their amenities. Merge this DataFrame\n",
    "#with the Titanic dataset to explore if access to amenities influenced survival rates. Perform an analysis using a “groupby()” \n",
    "#function to calculate survival rates based on amenities.\n",
    "\n",
    "# Mock DataFrame for cabin amenities\n",
    "cabin_amenities_df = pd.DataFrame({\n",
    "    'cabin': ['C85', 'C123', 'B42', 'E31', 'D56'],\n",
    "    'has_private_bathroom': [1, 1, 0, 0, 1],\n",
    "    'has_balcony': [1, 0, 1, 0, 0]\n",
    "})\n",
    "\n",
    "# Merge with Titanic dataset on 'cabin' column\n",
    "merged_df = pd.merge(df, cabin_amenities_df, on=\"cabin\", how=\"left\")\n",
    "\n",
    "# Analyze survival rates based on amenities\n",
    "amenity_survival_rates = merged_df.groupby(['has_private_bathroom', 'has_balcony'])['survived'].mean()\n",
    "\n",
    "print(amenity_survival_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bdba2",
   "metadata": {},
   "source": [
    "## 4. Advanced Data Cleaning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d47f7",
   "metadata": {},
   "source": [
    "1. String Manipulation:\n",
    "\n",
    "\"\"\"\n",
    "String manipulation refers to the process of modifying, analyzing, and transforming text data.\n",
    "It is commonly used in data cleaning, feature extraction, and text preprocessing.\n",
    "\n",
    "In Pandas, string manipulation is done using the `.str` accessor, which allows operations like:\n",
    "- Changing case: `.str.upper()`, `.str.lower()`\n",
    "- Removing whitespace: `.str.strip()`\n",
    "- Replacing text: `.str.replace()`\n",
    "- Extracting patterns: `.str.extract()`\n",
    "- Splitting text: `.str.split()`\n",
    "\n",
    "These functions are useful for handling messy text data, such as correcting formatting issues,\n",
    "extracting key information, and preparing text for analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fe9ea",
   "metadata": {},
   "source": [
    "2. Explain how Pandas can handle string manipulation for cleaning and\n",
    "transforming text data.\n",
    "\n",
    "How Pandas Handles String Manipulation\n",
    "\n",
    "\"\"\"\n",
    "Pandas provides powerful string manipulation functions via the `.str` accessor.\n",
    "It allows us to clean and transform text data in a DataFrame efficiently.\n",
    "\n",
    "Some common string operations in Pandas include:\n",
    "- `.str.lower()`, `.str.upper()`, `.str.capitalize()`: Changing letter case\n",
    "- `.str.strip()`: Removing whitespace\n",
    "- `.str.replace()`: Replacing substrings\n",
    "- `.str.contains()`: Checking for a substring\n",
    "- `.str.extract()`: Extracting patterns using regular expressions\n",
    "- `.str.split()`: Splitting strings based on a delimiter\n",
    "\n",
    "These functions are useful for cleaning messy datasets and extracting meaningful information.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "289ae4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cabin  Deck\n",
      "0   C85     C\n",
      "1   B20     B\n",
      "2  C123     C\n",
      "3   D10     D\n",
      "4  None  None\n"
     ]
    }
   ],
   "source": [
    "#3. Use Pandas’ string functions to extract the first letter from the 'Cabin' column to create a new column that \n",
    "#represents the deck of each passenger.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample Titanic dataset with 'Cabin' column\n",
    "data = {'PassengerId': [1, 2, 3, 4, 5],\n",
    "        'Cabin': ['C85', 'B20', 'C123', 'D10', None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extracting the first letter from the 'Cabin' column\n",
    "df['Deck'] = df['Cabin'].str[0]  # Gets the first character of each string\n",
    "\n",
    "print(df[['Cabin', 'Deck']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91ad78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deck\n",
      "A    0.500000\n",
      "B    0.723077\n",
      "C    0.606383\n",
      "D    0.695652\n",
      "E    0.731707\n",
      "F    0.619048\n",
      "G    0.600000\n",
      "T    0.000000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#4. Investigate whether passengers from different decks had different survival rates.\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "titanic_df = pd.read_csv(\"titanic_data_2.csv\")\n",
    "\n",
    "# Extracting deck information from 'Cabin'\n",
    "titanic_df['Deck'] = titanic_df['cabin'].str[0]  # Extracts first letter\n",
    "\n",
    "# Calculating survival rates by deck\n",
    "survival_rates = titanic_df.groupby('Deck')['survived'].mean()\n",
    "\n",
    "print(survival_rates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2c317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
